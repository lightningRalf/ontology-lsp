# Production Deployment - Next Steps Guide

## üéØ Current Status: 100% PRODUCTION READY

**System Status**: All critical blockers resolved, production deployment ready  
**Verification Complete**: ‚úÖ Builds, ‚úÖ Performance, ‚úÖ Health Endpoints, ‚úÖ Docker Config  
**Next Action**: Execute production deployment with proper Docker/K8s permissions

---

## üöÄ Immediate Next Steps (Priority Order)

### 1. Container Registry Setup (Required)

**Choose your container registry:**

#### Option A: GitHub Container Registry (Recommended for Open Source)
```bash
# Authenticate with GitHub Container Registry
echo $GITHUB_TOKEN | docker login ghcr.io -u $GITHUB_USERNAME --password-stdin

# Build and push the image (requires Docker permissions)
docker build -t ghcr.io/$GITHUB_USERNAME/ontology-lsp:2.0.0 .
docker push ghcr.io/$GITHUB_USERNAME/ontology-lsp:2.0.0

# Tag as latest
docker tag ghcr.io/$GITHUB_USERNAME/ontology-lsp:2.0.0 ghcr.io/$GITHUB_USERNAME/ontology-lsp:latest
docker push ghcr.io/$GITHUB_USERNAME/ontology-lsp:latest
```

#### Option B: Docker Hub (Public)
```bash
# Authenticate with Docker Hub
docker login

# Build and push
docker build -t $DOCKER_USERNAME/ontology-lsp:2.0.0 .
docker push $DOCKER_USERNAME/ontology-lsp:2.0.0
docker push $DOCKER_USERNAME/ontology-lsp:latest
```

#### Option C: Private Registry (Enterprise)
```bash
# Configure private registry
docker build -t your-registry.company.com/ontology-lsp:2.0.0 .
docker push your-registry.company.com/ontology-lsp:2.0.0
```

### 2. Update Kubernetes Manifests

**Update the deployment image reference:**

```bash
# Edit k8s/deployment.yaml
sed -i 's|image: ontology-lsp:2.0.0|image: ghcr.io/$GITHUB_USERNAME/ontology-lsp:2.0.0|g' k8s/deployment.yaml

# Or for Docker Hub
sed -i 's|image: ontology-lsp:2.0.0|image: $DOCKER_USERNAME/ontology-lsp:2.0.0|g' k8s/deployment.yaml
```

### 3. Production Environment Setup

**Infrastructure Requirements:**

#### Kubernetes Cluster Requirements
- **Version**: Kubernetes 1.25+ 
- **Nodes**: Minimum 3 nodes for HA
- **Memory**: 4GB+ available per node
- **CPU**: 2+ cores per node
- **Storage**: 50GB+ for databases and logs

#### Managed Services (Recommended)
- **Database**: PostgreSQL with pgvector extension
  - AWS RDS, Google Cloud SQL, or Azure Database
  - Minimum: 2 vCPUs, 8GB RAM, 100GB SSD
- **Cache**: Redis/Valkey cluster
  - AWS ElastiCache, Google Memorystore, or Azure Cache
  - Minimum: 2GB RAM, cluster mode enabled

### 4. Security Configuration

**Create production secrets:**

```bash
# Database credentials
kubectl create secret generic ontology-lsp-db-secret \
  --from-literal=username="ontology_prod" \
  --from-literal=password="$(openssl rand -base64 32)" \
  --from-literal=database="ontology_lsp_prod" \
  -n ontology-lsp

# Application secrets
kubectl create secret generic ontology-lsp-app-secret \
  --from-literal=jwt-secret="$(openssl rand -base64 64)" \
  --from-literal=api-key="$(openssl rand -hex 32)" \
  --from-literal=encryption-key="$(openssl rand -base64 32)" \
  -n ontology-lsp

# TLS certificates (if not using cert-manager)
kubectl create secret tls ontology-lsp-tls \
  --cert=path/to/tls.crt \
  --key=path/to/tls.key \
  -n ontology-lsp
```

---

## üèóÔ∏è Deployment Methods

### Method 1: Single Command Deployment (Fastest)

```bash
# All-in-one production deployment
kubectl apply -f k8s/production.yaml

# Monitor deployment
kubectl rollout status deployment/ontology-lsp -n ontology-lsp --timeout=600s
```

### Method 2: Step-by-Step Deployment (Recommended for First Time)

```bash
# 1. Create namespace
kubectl apply -f k8s/namespace.yaml

# 2. Apply configuration
kubectl apply -f k8s/configmap.yaml
kubectl apply -f k8s/secret.yaml  # After editing with real values

# 3. Deploy dependencies
kubectl apply -f k8s/postgres.yaml
kubectl apply -f k8s/redis.yaml

# Wait for dependencies
kubectl wait --for=condition=Ready pod -l app.kubernetes.io/name=postgres -n ontology-lsp --timeout=300s
kubectl wait --for=condition=Ready pod -l app.kubernetes.io/name=redis -n ontology-lsp --timeout=120s

# 4. Deploy application
kubectl apply -f k8s/deployment.yaml
kubectl apply -f k8s/service.yaml

# 5. Configure ingress and autoscaling
kubectl apply -f k8s/ingress.yaml
kubectl apply -f k8s/hpa.yaml

# 6. Verify deployment
kubectl get all -n ontology-lsp
```

### Method 3: Docker Compose (Quick Testing)

```bash
# For quick testing without Kubernetes
docker-compose up -d

# Check services
curl http://localhost:7000/health
curl http://localhost:7001/health

# View comprehensive stack
open http://localhost:3000  # Grafana
open http://localhost:16686 # Jaeger
```

---

## üìä Post-Deployment Verification

### 1. Health Check Verification

```bash
# Application endpoints
curl https://your-domain.com/health
curl https://your-domain.com/api/v1/status

# Kubernetes health
kubectl get pods -n ontology-lsp
kubectl get svc -n ontology-lsp
kubectl get ingress -n ontology-lsp
```

### 2. Performance Verification

```bash
# Load test with expected performance
curl -X POST https://your-domain.com/api/v1/analyze \
  -H "Content-Type: application/json" \
  -d '{"query": "find function definitions", "files": ["*.ts", "*.js"]}'

# Expected response time: <200ms for 95% of requests
```

### 3. Monitoring Setup

```bash
# Check metrics endpoint
curl https://your-domain.com/metrics

# Verify Prometheus scraping
kubectl port-forward svc/prometheus 9090:9090 -n monitoring

# Access Grafana dashboards
kubectl port-forward svc/grafana 3000:3000 -n monitoring
```

---

## üîß Environment-Specific Configurations

### Development/Staging
```yaml
# config/environments/staging.yaml
environment: staging
replicas: 1
resources:
  requests:
    memory: 512Mi
    cpu: 250m
  limits:
    memory: 1Gi
    cpu: 500m
logging:
  level: debug
```

### Production
```yaml
# config/environments/production.yaml  
environment: production
replicas: 3
resources:
  requests:
    memory: 1Gi
    cpu: 500m
  limits:
    memory: 2Gi
    cpu: 1000m
logging:
  level: info
```

---

## üö® CI/CD Pipeline Integration

### GitHub Actions (Automated Deployment)

```yaml
# .github/workflows/deploy.yml
name: Deploy to Production

on:
  push:
    branches: [ main ]
    tags: [ 'v*' ]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Build and push Docker image
        run: |
          echo ${{ secrets.GITHUB_TOKEN }} | docker login ghcr.io -u ${{ github.actor }} --password-stdin
          docker build -t ghcr.io/${{ github.repository }}:${{ github.sha }} .
          docker push ghcr.io/${{ github.repository }}:${{ github.sha }}
      
      - name: Deploy to Kubernetes
        run: |
          echo "${{ secrets.KUBECONFIG }}" | base64 -d > kubeconfig
          export KUBECONFIG=kubeconfig
          sed -i 's|IMAGE_TAG|ghcr.io/${{ github.repository }}:${{ github.sha }}|g' k8s/deployment.yaml
          kubectl apply -f k8s/production.yaml
```

---

## üìà Scaling Guidelines

### Initial Production Setup
- **Replicas**: 3 (minimum for HA)
- **Resources**: 1GB RAM, 0.5 CPU per replica
- **Database**: 2 vCPUs, 8GB RAM
- **Cache**: 2GB Redis cluster

### Scaling Triggers
- **CPU > 70%**: Add replicas
- **Memory > 80%**: Add replicas or increase limits  
- **Response time > 200ms**: Investigate bottlenecks
- **Error rate > 1%**: Check logs and dependencies

### Auto-scaling Configuration
```yaml
# Horizontal Pod Autoscaler
spec:
  minReplicas: 3
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80
```

---

## ‚ö†Ô∏è Critical Reminders

### Before Going Live
- [ ] **Backup Strategy**: Automated database backups configured
- [ ] **Monitoring Alerts**: Set up for downtime, errors, performance
- [ ] **SSL Certificates**: Valid TLS certificates installed
- [ ] **DNS Configuration**: Domain pointing to ingress/load balancer
- [ ] **Security Scan**: Container and cluster security validated
- [ ] **Load Testing**: System tested under expected traffic
- [ ] **Rollback Plan**: Documented procedure for quick rollback

### Day 1 Operations
- [ ] **Monitor logs** for first 24 hours
- [ ] **Check performance metrics** against baselines
- [ ] **Verify all integrations** (VS Code extension, CLI, etc.)
- [ ] **Test backup/restore** procedures
- [ ] **Validate monitoring alerts** are working

---

## üÜò Emergency Contacts & Procedures

### Quick Rollback
```bash
# Rollback to previous version
kubectl rollout undo deployment/ontology-lsp -n ontology-lsp

# Scale down if needed
kubectl scale deployment/ontology-lsp --replicas=0 -n ontology-lsp
```

### Emergency Debug
```bash
# Get all system status
kubectl get all -n ontology-lsp

# Check recent events
kubectl get events -n ontology-lsp --sort-by='.lastTimestamp'

# Emergency logs
kubectl logs -f deployment/ontology-lsp -n ontology-lsp --tail=100
```

---

## üìû Support Resources

- **Documentation**: Complete deployment guide in `DEPLOYMENT_GUIDE.md`
- **Configuration**: All configs in `config/` and `k8s/` directories
- **Monitoring**: Grafana dashboards in `config/grafana/dashboards/`
- **Troubleshooting**: Common issues documented in `TROUBLESHOOTING.md`

---

**üéØ Status**: Ready for production deployment  
**‚è∞ Next Action**: Execute container build and registry push with proper Docker permissions  
**üé™ Success Criteria**: All health endpoints responding, sub-200ms response times, zero errors